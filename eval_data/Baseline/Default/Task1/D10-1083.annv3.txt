Citance Number: 1 | Reference Article: D10-1083.txt | Citing Article: D11-1056.txt | Citation Marker Offset: ['264'] | Citation Marker: 2010 | Citation Offset: ['263','264'] | Citation Text: <S sid ="263" ssid = "44">Following Lee et al.</S><S sid ="264" ssid = "45">(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.</S> | Reference Offset: ['98'] | Reference Text: <S sid ="98" ssid = "3">Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: D10-1083.txt | Citing Article: D11-1059.txt | Citation Marker Offset: ['11'] | Citation Marker: 2010 | Citation Offset: ['10','11'] | Citation Text: <S sid ="10" ssid = "10">This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.</S><S sid ="11" ssid = "11">(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: D10-1083.txt | Citing Article: D11-1059.txt | Citation Marker Offset: ['17'] | Citation Marker: 2010 | Citation Offset: ['16','17'] | Citation Text: <S sid ="16" ssid = "16">More recently, Lee et al.</S><S sid ="17" ssid = "17">(2010) presented a new type-based model, and also reported very good results.</S> | Reference Offset: ['224'] | Reference Text: <S sid ="224" ssid = "85">Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: D10-1083.txt | Citing Article: D11-1059.txt | Citation Marker Offset: ['32'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['32'] | Citation Text: <S sid ="32" ssid = "32">As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.</S> | Reference Offset: ['224'] | Reference Text: <S sid ="224" ssid = "85">Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: D10-1083.txt | Citing Article: D11-1059.txt | Citation Marker Offset: ['91'] | Citation Marker: 2010 | Citation Offset: ['90','91'] | Citation Text: <S sid ="90" ssid = "57">2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.</S><S sid ="91" ssid = "58">(2010).</S> | Reference Offset: ['64'] | Reference Text: <S sid ="64" ssid = "13">Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then θt is drawn from DIRICHLET(α, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: D10-1083.txt | Citing Article: D11-1059.txt | Citation Marker Offset: ['130'] | Citation Marker: 2010 | Citation Offset: ['129','130'] | Citation Text: <S sid ="129" ssid = "35">Following Lee et al.</S><S sid ="130" ssid = "36">(2010) we used only the training sections for each language.</S> | Reference Offset: ['97'] | Reference Text: <S sid ="97" ssid = "2">During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: D10-1083.txt | Citing Article: D12-1086.txt | Citation Marker Offset: ['74'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['74'] | Citation Text: <S sid ="74" ssid = "33">Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)</S> | Reference Offset: ['37'] | Reference Text: <S sid ="37" ssid = "5">On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: D10-1083.txt | Citing Article: D12-1125.txt | Citation Marker Offset: ['213'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['213'] | Citation Text: <S sid ="213" ssid = "29">vised POS induction algorithm (Lee et al., 2010)</S> | Reference Offset: ['13'] | Reference Text: <S sid ="13" ssid = "13">In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: D10-1083.txt | Citing Article: D12-1127.txt | Citation Marker Offset: ['13'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['13'] | Citation Text: <S sid ="13" ssid = "13">Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).</S> | Reference Offset: ['13'] | Reference Text: <S sid ="13" ssid = "13">In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: D10-1083.txt | Citing Article: D13-1004.txt | Citation Marker Offset: ['27'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['27'] | Citation Text: <S sid ="27" ssid = "27">Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)</S> | Reference Offset: ['49'] | Reference Text: <S sid ="49" ssid = "17">Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: D10-1083.txt | Citing Article: N12-1045.txt | Citation Marker Offset: ['14'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['14'] | Citation Text: <S sid ="14" ssid = "14">Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)</S> | Reference Offset: ['49'] | Reference Text: <S sid ="49" ssid = "17">Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: D10-1083.txt | Citing Article: P11-1087.txt | Citation Marker Offset: ['41'] | Citation Marker: 2010 | Citation Offset: ['40','41','42','43'] | Citation Text: <S sid ="40" ssid = "20">Recently Lee et al.</S><S sid ="41" ssid = "21">(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.</S><S sid ="42" ssid = "22">However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.</S><S sid ="43" ssid = "23">(1992)â€™s one-class HMM.</S> | Reference Offset: ['97'] | Reference Text: <S sid ="97" ssid = "2">During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: D10-1083.txt | Citing Article: P11-1087.txt | Citation Marker Offset: ['153'] | Citation Marker: 2010 | Citation Offset: ['152','153','154'] | Citation Text: <S sid ="152" ssid = "33">It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.</S><S sid ="153" ssid = "34">(2010).</S><S sid ="154" ssid = "35">That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "4">The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: D10-1083.txt | Citing Article: P13-1150.txt | Citation Marker Offset: ['55'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['55'] | Citation Text: <S sid ="55" ssid = "27">Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)</S> | Reference Offset: ['81'] | Reference Text: <S sid ="81" ssid = "30">Specifically, the lexicon is generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: D10-1083.txt | Citing Article: W11-0301.txt | Citation Marker Offset: ['102'] | Citation Marker: Lee et al., 2010 | Citation Offset: ['102'] | Citation Text: <S sid ="102" ssid = "55">Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).</S> | Reference Offset: ['64'] | Reference Text: <S sid ="64" ssid = "13">Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then θt is drawn from DIRICHLET(α, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: D10-1083.txt | Citing Article: W12-1914.txt | Citation Marker Offset: ['8'] | Citation Marker: 2010 | Citation Offset: ['7','8'] | Citation Text: <S sid ="7" ssid = "7">Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.</S><S sid ="8" ssid = "8">(2010), Lamar et al.</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi and Knight, 2009).</S> | Discourse Facet: BLANK | Annotator: Predictions

