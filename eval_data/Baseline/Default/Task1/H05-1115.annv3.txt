Citance Number: 1 | Reference Article: H05-1115.xml | Citing Article: P09-1083.xml | Citation Marker Offset: ['60'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['60'] | Citation Text: <S sid ="60" ssid = "6">Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: H05-1115.xml | Citing Article: P09-1083.xml | Citation Marker Offset: ['85'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['85'] | Citation Text: <S sid ="85" ssid = "31">Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: H05-1115.xml | Citing Article: P09-1083.xml | Citation Marker Offset: ['43'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['43'] | Citation Text: <S sid ="43" ssid = "14">includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).</S> | Reference Offset: ['31'] | Reference Text: <S sid ="31" ssid = "31">Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: H05-1115.xml | Citing Article: N06-1027.xml | Citation Marker Offset: ['41'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['41'] | Citation Text: <S sid ="41" ssid = "7">and sentence retrieval for question answering (Otterbacher et al., 2005).</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: H05-1115.xml | Citing Article: C08-1062.xml | Citation Marker Offset: ['83'] | Citation Marker: OtterBacher et al., 2005 | Citation Offset: ['83'] | Citation Text: <S sid ="83" ssid = "83">These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).</S> | Reference Offset: ['187'] | Reference Text: <S sid ="187" ssid = "15">As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: H05-1115.xml | Citing Article: C10-2049.xml | Citation Marker Offset: ['113'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['113'] | Citation Text: <S sid ="113" ssid = "31">Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: H05-1115.xml | Citing Article: D08-1032.xml | Citation Marker Offset: ['24'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['24','25'] | Citation Text: <S sid ="24" ssid = "3">A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.</S><S sid ="25" ssid = "4">Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: H05-1115.xml | Citing Article: D08-1032.xml | Citation Marker Offset: ['94'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['94'] | Citation Text: <S sid ="94" ssid = "73">To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).</S> | Reference Offset: ['187'] | Reference Text: <S sid ="187" ssid = "15">As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: H05-1115.xml | Citing Article: P08-2003.xml | Citation Marker Offset: ['11'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['11'] | Citation Text: <S sid ="11" ssid = "11">A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: H05-1115.xml | Citing Article: P08-2003.xml | Citation Marker Offset: ['22'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['22'] | Citation Text: <S sid ="22" ssid = "5">To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).</S> | Reference Offset: ['187'] | Reference Text: <S sid ="187" ssid = "15">As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: H05-1115.xml | Citing Article: P10-2055.xml | Citation Marker Offset: ['108'] | Citation Marker: Otterbacher et al., 2005 | Citation Offset: ['104','108'] | Citation Text: <S sid ="104" ssid = "36">Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.</S><S sid ="108" ssid = "40">The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "53">Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&amp;A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.</S> | Discourse Facet: BLANK | Annotator: Predictions

