Citance Number: 2 | Reference Article: P98-1081.xml | Citing Article: W01-0712.xml | Citation Marker Offset: ['130'] | Citation Marker: van Halteren et al., 1998 | Citation Offset: ['130'] | Citation Text: <S sid ="130" ssid = "116">And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: P98-1081.xml | Citing Article: W01-0712.xml | Citation Marker Offset: ['135'] | Citation Marker: 1998 | Citation Offset: ['134','135'] | Citation Text: <S sid ="134" ssid = "120">Like Van Halteren et al.</S><S sid ="135" ssid = "121">(1998), we evaluated two features combinations.</S> | Reference Offset: ['133'] | Reference Text: <S sid ="133" ssid = "54">For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: P98-1081.xml | Citing Article: W02-1004.xml | Citation Marker Offset: ['105'] | Citation Marker: 1998 | Citation Offset: ['105','106','107','108','109','110','111'] | Citation Text: <S sid ="105" ssid = "48">Van Halteren et al.</S><S sid ="106" ssid = "49">(1998) introduce a modi.ed version of voting called TagPair.</S><S sid ="107" ssid = "50">Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid ="108" ssid = "51">j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid ="109" ssid = "52">Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.</S><S sid ="110" ssid = "53">In the experi­ments presented in van Halteren et al.</S><S sid ="111" ssid = "54">(1998), this method was the best performer among the presented methods.</S> | Reference Offset: ['154'] | Reference Text: <S sid ="154" ssid = "75">To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: P98-1081.xml | Citing Article: E99-1025.xml | Citation Marker Offset: ['118'] | Citation Marker: 1998 | Citation Offset: ['117','118'] | Citation Text: <S sid ="117" ssid = "73">We consider three voting strategies suggested by van Halteren et al.</S><S sid ="118" ssid = "74">(1998): equal vote, where each classifier&apos;s vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&apos;wise voting.</S> | Reference Offset: ['118'] | Reference Text: <S sid ="118" ssid = "39">When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: P98-1081.xml | Citing Article: P06-2060.xml | Citation Marker Offset: ['35'] | Citation Marker: 1998 | Citation Offset: ['35'] | Citation Text: <S sid ="35" ssid = "13">Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.</S> | Reference Offset: ['31'] | Reference Text: <S sid ="31" ssid = "31">Second, current performance levels on this task still leave room for improvement: &apos;state of the art&apos; performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: P98-1081.xml | Citing Article: A00-1024.xml | Citation Marker Offset: ['43'] | Citation Marker: van Halteren et al., 1998 | Citation Offset: ['42','43'] | Citation Text: <S sid ="42" ssid = "17">Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.</S><S sid ="43" ssid = "18">(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).</S> | Reference Offset: ['154'] | Reference Text: <S sid ="154" ssid = "75">To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: P98-1081.xml | Citing Article: W05-1518.xml | Citation Marker Offset: ['10'] | Citation Marker: van Halteren et ... al., 1998 | Citation Offset: ['10','11'] | Citation Text: <S sid ="10" ssid = "10">Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).</S><S sid ="11" ssid = "11">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S> | Reference Offset: ['80'] | Reference Text: <S sid ="80" ssid = "1">In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: P98-1081.xml | Citing Article: W05-1518.xml | Citation Marker Offset: ['82'] | Citation Marker: 1998 | Citation Offset: ['81','82','83'] | Citation Text: <S sid ="81" ssid = "19">Van Halteren et al.</S><S sid ="82" ssid = "20">(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.</S><S sid ="83" ssid = "21">The vote of each classifier (parser) is weighted by their respective accuracy.</S> | Reference Offset: ['118'] | Reference Text: <S sid ="118" ssid = "39">When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: P98-1081.xml | Citing Article: W05-1518.xml | Citation Marker Offset: ['90'] | Citation Marker: van Halteren et al., 1998 | Citation Offset: ['90'] | Citation Text: <S sid ="90" ssid = "28">Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.</S> | Reference Offset: ['154'] | Reference Text: <S sid ="154" ssid = "75">To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: P98-1081.xml | Citing Article: W05-1518.xml | Citation Marker Offset: ['111'] | Citation Marker: van Halteren et al., 1998 | Citation Offset: ['111'] | Citation Text: <S sid ="111" ssid = "3">In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: P98-1081.xml | Citing Article: W00-0733.xml | Citation Marker Offset: ['26'] | Citation Marker: 1998 | Citation Offset: ['26'] | Citation Text: <S sid ="26" ssid = "23">The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).</S> | Reference Offset: ['154'] | Reference Text: <S sid ="154" ssid = "75">To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: Citation Number: 16 | Reference Article: P98-1081.xml | Citing Article: W00-0733.xml | Citation Marker Offset: ['18'] | Citation Marker: Van Halteren et al., 1998 | Citation Offset: ['18','19','20'] | Citation Text: <S sid ="18" ssid = "15">We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).</S><S sid ="19" ssid = "16">Five are so-called voting methods.</S><S sid ="20" ssid = "17">They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.</S> | Reference Offset: ['49'] | Reference Text: <S sid ="49" ssid = "49">Z. system starts with a basic corpus annotation (each word is tagged with its most likely tag) and then searches through a space of transformation rules in order to reduce the discrepancy between its current annotation and the correct one (in our case 528 rules were learned).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: P98-1081.xml | Citing Article: W00-0733.xml | Citation Marker Offset: ['32'] | Citation Marker: Van Halteren et al., 1998 | Citation Offset: ['32'] | Citation Text: <S sid ="32" ssid = "29">For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['40'] | Citation Marker: van Halteren, Zavrel, and Daelemans 1998 | Citation Offset: ['40','41','42'] | Citation Text: <S sid ="40" ssid = "40">First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).</S><S sid ="41" ssid = "41">However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.</S><S sid ="42" ssid = "42">This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 19 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['183'] | Citation Marker: 1998 | Citation Offset: ['183','184','185','186'] | Citation Text: <S sid ="183" ssid = "33">Compare this to the &quot;tune&quot; set in van Halteren, Zavrel, and Daelemans (1998).</S><S sid ="184" ssid = "34">This consisted of 114K.</S><S sid ="185" ssid = "35">tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.</S><S sid ="186" ssid = "36">This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.</S> | Reference Offset: ['72'] | Reference Text: <S sid ="72" ssid = "10">The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if &quot;as well as&quot; is taken to be a coordination conjunction, it is tagged &quot;as_CC1 well_CC2 as_CC3&quot;, using three related but different ditto tags.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 20 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['570'] | Citation Marker: 1998 | Citation Offset: ['570','571','573'] | Citation Text: <S sid ="570" ssid = "7">For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).</S><S sid ="571" ssid = "8">In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.</S><S sid ="573" ssid = "10">As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.</S> | Reference Offset: ['72'] | Reference Text: <S sid ="72" ssid = "10">The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if &quot;as well as&quot; is taken to be a coordination conjunction, it is tagged &quot;as_CC1 well_CC2 as_CC3&quot;, using three related but different ditto tags.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 21 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['102'] | Citation Marker: van Halteren, Zavrel, and Daelemans 1998 | Citation Offset: ['102','103','104'] | Citation Text: <S sid ="102" ssid = "52">One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.</S><S sid ="103" ssid = "53">It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.</S><S sid ="104" ssid = "54">Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.</S> | Reference Offset: ['117'] | Reference Text: <S sid ="117" ssid = "38">We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 22 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['487'] | Citation Marker: 1998 | Citation Offset: ['487','488'] | Citation Text: <S sid ="487" ssid = "81">The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.</S><S sid ="488" ssid = "82">Where TagPair used to be significantly better than MBL, the roles are now well reversed.</S> | Reference Offset: ['72'] | Reference Text: <S sid ="72" ssid = "10">The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if &quot;as well as&quot; is taken to be a coordination conjunction, it is tagged &quot;as_CC1 well_CC2 as_CC3&quot;, using three related but different ditto tags.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 23 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['176'] | Citation Marker: van Halteren, Zavrel, and Daelemans 1998 | Citation Offset: ['176'] | Citation Text: <S sid ="176" ssid = "26">The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.</S> | Reference Offset: ['154'] | Reference Text: <S sid ="154" ssid = "75">To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 24 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['309'] | Citation Marker: 1998 | Citation Offset: ['309'] | Citation Text: <S sid ="309" ssid = "159">In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods.</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 25 | Reference Article: P98-1081.xml | Citing Article: J01-2002.xml | Citation Marker Offset: ['398'] | Citation Marker: van Halteren,Zavrel, and Daelemans 1998 | Citation Offset: ['398'] | Citation Text: <S sid ="398" ssid = "83">With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word</S> | Reference Offset: ['59'] | Reference Text: <S sid ="59" ssid = "59">It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).</S> | Discourse Facet: BLANK | Annotator: Predictions

