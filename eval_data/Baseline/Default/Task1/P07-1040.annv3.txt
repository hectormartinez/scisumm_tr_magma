Citance Number: 1 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '17' | Citation Marker: Rosti et al.,2007b | Citation Offset: '16','17' | Citation Text: <S sid ="16" ssid = "16">In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.</S><S sid ="17" ssid = "17">This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '32' | Citation Marker: Rosti et al.,2007b | Citation Offset: '32' | Citation Text: <S sid ="32" ssid = "7">Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).</S> | Reference Offset: ['135'] | Reference Text: <S sid ="135" ssid = "14">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '92' | Citation Marker: Rosti et al.,2007b | Citation Offset: '88','89','90','91','92' | Citation Text: <S sid ="88" ssid = "35">Bangalore et al.</S><S sid ="89" ssid = "36">(2001), Sim et al.</S><S sid ="90" ssid = "37">(2007), Rosti et al.</S><S sid ="91" ssid = "38">(2007a), and Rosti et al.</S><S sid ="92" ssid = "39">(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "20">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '97' | Citation Marker: Rosti et al.,2007b | Citation Offset: '93','94','95','96','97' | Citation Text: <S sid ="93" ssid = "40">Bangalore et al.</S><S sid ="94" ssid = "41">(2001) used a WER based alignment and Sim et al.</S> <S sid ="95" ssid = "42">(2007), Rosti et al.</S><S sid ="96" ssid = "43">(2007a), and Rosti et al.</S><S sid ="97" ssid = "44">(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.</S> | Reference Offset: ['153'] | Reference Text: <S sid ="153" ssid = "1">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '7' | Citation Marker: Rosti et al.,2007a | Citation Offset: '7' | Citation Text: <S sid ="7" ssid = "7">In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "20">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '133', '134' | Citation Marker: Rosti et al.,2007a | Citation Offset: '133','134' | Citation Text: <S sid ="133" ssid = "3">While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.</S><S sid ="134" ssid = "4">(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.</S> | Reference Offset: ['111'] | Reference Text: <S sid ="111" ssid = "7">First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '148' | Citation Marker: Rosti et al.,2007a | Citation Offset: '147','148' | Citation Text: <S sid ="147" ssid = "17">Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).</S><S sid ="148" ssid = "18">ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).</S> | Reference Offset: ['106'] | Reference Text: <S sid ="106" ssid = "2">Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: P07-1040 | Citing Article: N09-2003 | Citation Marker Offset: '24', '25' | Citation Marker: 2007.0 | Citation Offset: '24','25' | Citation Text: <S sid ="24" ssid = "24">Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.</S><S sid ="25" ssid = "25">(2007), each component system produces a set of translations, which are then grafted to form a confusion network.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "20">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '2', '3' | Citation Marker: 2007.0 | Citation Offset: '2','3' | Citation Text: <S sid ="2" ssid = "2">We build our confusion networks using the method of Rosti et al.</S><S sid ="3" ssid = "3">(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.</S> | Reference Offset: ['24'] | Reference Text: <S sid ="24" ssid = "24">A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '23', '24' | Citation Marker: 2007.0 | Citation Offset: '23','24' | Citation Text: <S sid ="23" ssid = "23">The procedure described by Rosti et al.</S> <S sid ="24" ssid = "24">(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.</S> | Reference Offset: ['24'] | Reference Text: <S sid ="24" ssid = "24">A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '29', '30' | Citation Marker: 2007.0 | Citation Offset: '28','29','30' | Citation Text: <S sid ="28" ssid = "28">In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.</S><S sid ="29" ssid = "29">For this, Rosti et al.</S><S sid ="30" ssid = "30">(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.</S> | Reference Offset: ['24'] | Reference Text: <S sid ="24" ssid = "24">A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '69', '70' | Citation Marker: 2007.0 | Citation Offset: '69','70' | Citation Text: <S sid ="69" ssid = "1">ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.</S><S sid ="70" ssid = "2">(2007).</S> | Reference Offset: ['135'] | Reference Text: <S sid ="135" ssid = "14">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '78', '79' | Citation Marker: 2007.0 | Citation Offset: '78','79' | Citation Text: <S sid ="78" ssid = "10">Note that the algorithm of Rosti et al.</S><S sid ="79" ssid = "11">(2007) used N -best lists in the combination.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "20">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: P07-1040 | Citing Article: P11-1125 | Citation Marker Offset: '47', '48' | Citation Marker: 2007b | Citation Offset: '46','47','48' | Citation Text: <S sid ="46" ssid = "16">This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).</S><S sid ="47" ssid = "17">Rosti et al.</S><S sid ="48" ssid = "18">(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.</S> | Reference Offset: ['135'] | Reference Text: <S sid ="135" ssid = "14">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: P07-1040 | Citing Article: P11-1125 | Citation Marker Offset: '127' | Citation Marker: 2007b | Citation Offset: '127' | Citation Text: <S sid ="127" ssid = "18">Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).</S> | Reference Offset: ['135'] | Reference Text: <S sid ="135" ssid = "14">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: P07-1040 | Citing Article: P39_p07 | Citation Marker Offset: '24', '25' | Citation Marker: 2007.0 | Citation Offset: '24','25' | Citation Text: <S sid ="24" ssid = "24">@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.</S><S sid ="25" ssid = "25">(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "20">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: P07-1040 | Citing Article: P101121_p07 | Citation Marker Offset: '20', '21' | Citation Marker: 2007.0 | Citation Offset: ['19','20','21'] | Citation Text: <S sid ="19" ssid = "19">If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.</S> <S sid ="20" ssid = "20">For example, Rosti et al.</S><S sid ="21" ssid = "21">(2007) report such an effect.</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: P07-1040 | Citing Article: Pling_p07 | Citation Marker Offset: '13' | Citation Marker: 2007.0 | Citation Offset: '12','13' | Citation Text: <S sid ="12" ssid = "12">In this paper, a system combination based on confusion network (CN) is described.</S><S sid ="13" ssid = "13">This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).</S> | Reference Offset: ['153'] | Reference Text: <S sid ="153" ssid = "1">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 19 | Reference Article: P07-1040 | Citing Article: Pling_p07 | Citation Marker Offset: '41' | Citation Marker: 2007.0 | Citation Offset: '41' | Citation Text: <S sid ="41" ssid = "13">This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 20 | Reference Article: P07-1040 | Citing Article: Psem_p07 | Citation Marker Offset: '13' | Citation Marker: Rosti et al. 2007 | Citation Offset: '13' | Citation Text: <S sid ="13" ssid = "13">The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 21 | Reference Article: P07-1040 | Citing Article: Psem_p07 | Citation Marker Offset: '16' | Citation Marker: Rosti et al. 2007 | Citation Offset: '116' | Citation Text: <S sid ="116" ssid = "8">In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 22 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '7' | Citation Marker: Rosti et al. 2007 | Citation Offset: '7' | Citation Text: <S sid ="7" ssid = "7">The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).</S> | Reference Offset: ['24'] | Reference Text: <S sid ="24" ssid = "24">A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 23 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '17' | Citation Marker: Rosti et al. 2007 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "21">In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 24 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '39' | Citation Marker: Rosti et al. 2007 | Citation Offset: '39' | Citation Text: <S sid ="39" ssid = "17">Other scores for the word arc are set as in (Rosti et al., 2007).</S> | Reference Offset: ['34'] | Reference Text: <S sid ="34" ssid = "34">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 25 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '88' | Citation Marker: Rosti et al. 2007 | Citation Offset: '87','88' | Citation Text: <S sid ="87" ssid = "21">The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.</S><S sid ="88" ssid = "22">to the pairwise TER alignment described in (Rosti et al., 2007).</S> | Reference Offset: ['175'] | Reference Text: <S sid ="175" ssid = "23">This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 26 | Reference Article: P07-1040 | Citing Article: W09-0441 | Citation Marker Offset: '36' | Citation Marker: Rosti et al.,2007 | Citation Offset: '35','36' | Citation Text: <S sid ="35" ssid = "9">The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.</S><S sid ="36" ssid = "10">Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).</S> | Reference Offset: ['92'] | Reference Text: <S sid ="92" ssid = "13">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

