Citance Number: 1 | Reference Article: W95-0104.txt | Citing Article: A00-2019.txt | Citation Marker Offset: ['23'] | Citation Marker: 1995 | Citation Offset: ['23'] | Citation Text: <S sid ="23" ssid = "23">Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&apos;re.</S> | Reference Offset: ['30'] | Reference Text: <S sid ="30" ssid = "2">Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: W95-0104.txt | Citing Article: A97-1025.txt | Citation Marker Offset: ['29'] | Citation Marker: Golding, 1995 | Citation Offset: ['29'] | Citation Text: <S sid ="29" ssid = "7">A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).</S> | Reference Offset: ['27'] | Reference Text: <S sid ="27" ssid = "27">The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: W95-0104.txt | Citing Article: A97-1025.txt | Citation Marker Offset: ['125'] | Citation Marker: 1995 | Citation Offset: ['125'] | Citation Text: <S sid ="125" ssid = "1">The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).</S> | Reference Offset: ['320'] | Reference Text: <S sid ="320" ssid = "36">We tried Schabes&apos;s method on the usual confusion sets; the results are in the last column of Table 7.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: W95-0104.txt | Citing Article: C04-1131.txt | Citation Marker Offset: ['46'] | Citation Marker: Golding, 1995 | Citation Offset: ['46'] | Citation Text: <S sid ="46" ssid = "34">We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).</S> | Reference Offset: ['255'] | Reference Text: <S sid ="255" ssid = "204">There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: W95-0104.txt | Citing Article: D07-1012.txt | Citation Marker Offset: ['71'] | Citation Marker: 1995 | Citation Offset: ['71'] | Citation Text: <S sid ="71" ssid = "36">Golding (1995) builds a classifier based on a rich set of context features.</S> | Reference Offset: ['158'] | Reference Text: <S sid ="158" ssid = "107">two ladies who lay pinkly nude beside him in the desert Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each word in the sentence with its set of possible part-of-speech tags, obtained from a dictionary.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: W95-0104.txt | Citing Article: D11-1119.txt | Citation Marker Offset: ['40'] | Citation Marker: Golding, 1995 | Citation Offset: ['40'] | Citation Text: <S sid ="40" ssid = "11">A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)</S> | Reference Offset: ['334'] | Reference Text: <S sid ="334" ssid = "5">It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: W95-0104.txt | Citing Article: E06-1030.txt | Citation Marker Offset: ['164'] | Citation Marker: 1995 | Citation Offset: ['164'] | Citation Text: <S sid ="164" ssid = "24">The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "65">\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: W95-0104.txt | Citing Article: E99-1024.txt | Citation Marker Offset: ['34'] | Citation Marker: Golding, 1995 | Citation Offset: ['33','34'] | Citation Text: <S sid ="33" ssid = "33">Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.</S><S sid ="34" ssid = "34">For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).</S> | Reference Offset: ['74'] | Reference Text: <S sid ="74" ssid = "23">The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; &apos;s distribution they most closely follow.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: W95-0104.txt | Citing Article: H01-1052.txt | Citation Marker Offset: ['20'] | Citation Marker: 3 | Citation Offset: ['20','21','22'] | Citation Text: <S sid ="20" ssid = "3">The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].</S><S sid ="21" ssid = "4">In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.</S><S sid ="22" ssid = "5">Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.</S> | Reference Offset: ['79'] | Reference Text: <S sid ="79" ssid = "28">57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u&apos;r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y&apos; re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: W95-0104.txt | Citing Article: J98-1006.txt | Citation Marker Offset: ['108'] | Citation Marker: 1995 | Citation Offset: ['108','109'] | Citation Text: <S sid ="108" ssid = "80">For each si, the probability is computed with Bayes&apos; rule: As Golding (1995) points out, the term p(c_kf ..</S><S sid ="109" ssid = "81">.,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then</S> | Reference Offset: ['62'] | Reference Text: <S sid ="62" ssid = "11">For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o·r left as) desert.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: W95-0104.txt | Citing Article: N03-2035.txt | Citation Marker Offset: ['23'] | Citation Marker: 3 | Citation Offset: ['23','24'] | Citation Text: <S sid ="23" ssid = "23">Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.</S><S sid ="24" ssid = "24">The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: W95-0104.txt | Citing Article: N03-2035.txt | Citation Marker Offset: ['56'] | Citation Marker: 3 | Citation Offset: ['56'] | Citation Text: <S sid ="56" ssid = "23">Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.</S> | Reference Offset: ['73'] | Reference Text: <S sid ="73" ssid = "22">On the other hand, words such as chocolate and delicious in the context imply desser·t. This observation is the basis for the method of context words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: W95-0104.txt | Citing Article: N03-2035.txt | Citation Marker Offset: ['89'] | Citation Marker: 3 | Citation Offset: ['89'] | Citation Text: <S sid ="89" ssid = "56">In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.</S> | Reference Offset: ['271'] | Reference Text: <S sid ="271" ssid = "220">Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it&apos;s}.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: W95-0104.txt | Citing Article: N04-1016.txt | Citation Marker Offset: ['98'] | Citation Marker: Golding, 1995 | Citation Offset: ['98','99'] | Citation Text: <S sid ="98" ssid = "8">These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).</S><S sid ="99" ssid = "9">Despite their differences, most approaches use two types of features: context words and collocations.</S> | Reference Offset: ['255'] | Reference Text: <S sid ="255" ssid = "204">There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: W95-0104.txt | Citing Article: N04-1016.txt | Citation Marker Offset: ['102'] | Citation Marker: 1995 | Citation Offset: ['102'] | Citation Text: <S sid ="102" ssid = "12">All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).</S> | Reference Offset: ['160'] | Reference Text: <S sid ="160" ssid = "109">The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: W95-0104.txt | Citing Article: N04-1016.txt | Citation Marker Offset: ['103'] | Citation Marker: 1995 | Citation Offset: ['103'] | Citation Text: <S sid ="103" ssid = "13">Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.</S> | Reference Offset: ['79'] | Reference Text: <S sid ="79" ssid = "28">57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u&apos;r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y&apos; re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: W95-0104.txt | Citing Article: N04-1016.txt | Citation Marker Offset: ['114'] | Citation Marker: 1995 | Citation Offset: ['114'] | Citation Text: <S sid ="114" ssid = "24">Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.</S> | Reference Offset: ['59'] | Reference Text: <S sid ="59" ssid = "8">and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: W95-0104.txt | Citing Article: N04-1016.txt | Citation Marker Offset: ['116'] | Citation Marker: 1995 | Citation Offset: ['116'] | Citation Text: <S sid ="116" ssid = "26">A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).</S> | Reference Offset: ['116'] | Reference Text: <S sid ="116" ssid = "65">\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 19 | Reference Article: W95-0104.txt | Citing Article: N10-1019.txt | Citation Marker Offset: ['11'] | Citation Marker: 1995 | Citation Offset: ['11'] | Citation Text: <S sid ="11" ssid = "11">The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).</S> | Reference Offset: ['106'] | Reference Text: <S sid ="106" ssid = "55">To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 20 | Reference Article: W95-0104.txt | Citing Article: P01-1005.txt | Citation Marker Offset: ['19'] | Citation Marker: Golding, 1995 | Citation Offset: ['19','20'] | Citation Text: <S sid ="19" ssid = "4">The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).</S><S sid ="20" ssid = "5">In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.</S> | Reference Offset: ['255'] | Reference Text: <S sid ="255" ssid = "204">There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 21 | Reference Article: W95-0104.txt | Citing Article: P96-1010.txt | Citation Marker Offset: ['25'] | Citation Marker: Golding, 1995 | Citation Offset: ['25'] | Citation Text: <S sid ="25" ssid = "25">Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">This paper takes Yarowsky&apos;s work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 22 | Reference Article: W95-0104.txt | Citing Article: P96-1010.txt | Citation Marker Offset: ['38'] | Citation Marker: Golding, 1995 | Citation Offset: ['38'] | Citation Text: <S sid ="38" ssid = "38">We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.</S> | Reference Offset: ['328'] | Reference Text: <S sid ="328" ssid = "44">This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 23 | Reference Article: W95-0104.txt | Citing Article: P96-1010.txt | Citation Marker Offset: ['78'] | Citation Marker: Golding, 1995 | Citation Offset: ['78'] | Citation Text: <S sid ="78" ssid = "3">A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).</S> | Reference Offset: ['335'] | Reference Text: <S sid ="335" ssid = "6">A comparison of the Bayesian hybrid method with Schabes&apos;s trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 24 | Reference Article: W95-0104.txt | Citing Article: P96-1010.txt | Citation Marker Offset: ['80'] | Citation Marker: Golding, 1995 | Citation Offset: ['79','80'] | Citation Text: <S sid ="79" ssid = "4">We adopt the Bayesian hybrid method</S><S sid ="80" ssid = "5">This method has been described elsewhere (Golding, 1995)</S> | Reference Offset: ['62'] | Reference Text: <S sid ="62" ssid = "11">For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o·r left as) desert.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 25 | Reference Article: W95-0104.txt | Citing Article: P98-2138.txt | Citation Marker Offset: ['95'] | Citation Marker: Golding, 1995 | Citation Offset: ['95'] | Citation Text: <S sid ="95" ssid = "13">For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).</S> | Reference Offset: ['40'] | Reference Text: <S sid ="40" ssid = "12">A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 26 | Reference Article: W95-0104.txt | Citing Article: P98-2138.txt | Citation Marker Offset: ['143'] | Citation Marker: Golding, 1995 | Citation Offset: ['143'] | Citation Text: <S sid ="143" ssid = "61">Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.</S> | Reference Offset: ['27'] | Reference Text: <S sid ="27" ssid = "27">The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 27 | Reference Article: W95-0104.txt | Citing Article: W00-0701.txt | Citation Marker Offset: ['23'] | Citation Marker: Gol95 | Citation Offset: ['23'] | Citation Text: <S sid ="23" ssid = "6">This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).</S> | Reference Offset: ['40'] | Reference Text: <S sid ="40" ssid = "12">A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 28 | Reference Article: W95-0104.txt | Citing Article: W00-0701.txt | Citation Marker Offset: ['113'] | Citation Marker: Gol95 | Citation Offset: ['113'] | Citation Text: <S sid ="113" ssid = "18">MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).</S> | Reference Offset: ['255'] | Reference Text: <S sid ="255" ssid = "204">There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 29 | Reference Article: W95-0104.txt | Citing Article: W01-0502.txt | Citation Marker Offset: ['10'] | Citation Marker: Golding, 1995 | Citation Offset: ['10'] | Citation Text: <S sid ="10" ssid = "10">A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)</S> | Reference Offset: ['231'] | Reference Text: <S sid ="231" ssid = "180">Besides the reliability metric, therefore, we also considered an alternative metric: the uncertainty coefficient of x, denoted U(xiy) [Press et al., 1988, p..501].</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 30 | Reference Article: W95-0104.txt | Citing Article: W02-1005.txt | Citation Marker Offset: ['9'] | Citation Marker: Golding, 1995 | Citation Offset: ['9'] | Citation Text: <S sid ="9" ssid = "9">Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)</S> | Reference Offset: ['187'] | Reference Text: <S sid ="187" ssid = "136">We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 32 | Reference Article: W95-0104.txt | Citing Article: W02-1005.txt | Citation Marker Offset: ['180'] | Citation Marker: 1995 | Citation Offset: ['180'] | Citation Text: <S sid ="180" ssid = "30">For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)</S> | Reference Offset: ['39'] | Reference Text: <S sid ="39" ssid = "11">Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of &quot;\Vords Commonly Confused&quot; in the back of the Random House unabridged dictionary [Fiexner, 1983].</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 33 | Reference Article: W95-0104.txt | Citing Article: W04-3238.txt | Citation Marker Offset: ['13'] | Citation Marker: Golding, 1995 | Citation Offset: ['13'] | Citation Text: <S sid ="13" ssid = "13">A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).</S> | Reference Offset: ['271'] | Reference Text: <S sid ="271" ssid = "220">Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it&apos;s}.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 34 | Reference Article: W95-0104.txt | Citing Article: W06-1624.txt | Citation Marker Offset: ['84'] | Citation Marker: Golding, 1995 | Citation Offset: ['84'] | Citation Text: <S sid ="84" ssid = "60">We use the metric described in (Yarowsky, 1994; Golding, 1995).</S> | Reference Offset: ['160'] | Reference Text: <S sid ="160" ssid = "109">The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 35 | Reference Article: W95-0104.txt | Citing Article: W06-3604.txt | Citation Marker Offset: ['146'] | Citation Marker: Golding, 1995 | Citation Offset: ['146'] | Citation Text: <S sid ="146" ssid = "9">More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">This paper takes Yarowsky&apos;s work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 37 | Reference Article: W95-0104.txt | Citing Article: W12-0304.txt | Citation Marker Offset: ['33'] | Citation Marker: Golding, 1995 | Citation Offset: ['33'] | Citation Text: <S sid ="33" ssid = "7">There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.</S> | Reference Offset: ['255'] | Reference Text: <S sid ="255" ssid = "204">There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 38 | Reference Article: W95-0104.txt | Citing Article: W96-0108.txt | Citation Marker Offset: ['28'] | Citation Marker: 1995 | Citation Offset: ['28'] | Citation Text: <S sid ="28" ssid = "28">Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.</S> | Reference Offset: ['328'] | Reference Text: <S sid ="328" ssid = "44">This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 39 | Reference Article: W95-0104.txt | Citing Article: W98-1234.txt | Citation Marker Offset: ['4'] | Citation Marker: 3 | Citation Offset: ['4'] | Citation Text: <S sid ="4" ssid = "4">Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).</S> | Reference Offset: ['79'] | Reference Text: <S sid ="79" ssid = "28">57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u&apos;r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y&apos; re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

