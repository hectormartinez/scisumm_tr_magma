Citance Number: 1 | Reference Article: J98-2005.xml | Citing Article: J01-2004.xml | Citation Marker Offset: ['72'] | Citation Marker: 1998 | Citation Offset: ['72'] | Citation Text: <S sid ="72" ssid = "37">Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.</S> | Reference Offset: ['42'] | Reference Text: <S sid ="42" ssid = "6">(A~c~)ER Chi and Geman Probabilistic Context-Free Grammars Given a set of finite parse trees cab ca2,..., can, drawn independently according to the distribution imposed by p, we wish to estimate p. In terms of the frequency function f, introduced in Section 1, the likelihood of the data is L = L(p;cal,ca2 ..... con) n = II II p(AY i=1 (A~)ER Recall the derivation of the maximum-likelihood estimator of p: The log of the likelihood is: n ~ ~f(A --+ a;cai)logp(A ~ a).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: J98-2005.xml | Citing Article: N03-1027.xml | Citation Marker Offset: ['56'] | Citation Marker: 1998 | Citation Offset: ['56'] | Citation Text: <S sid ="56" ssid = "19">Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.</S> | Reference Offset: ['20'] | Reference Text: <S sid ="20" ssid = "20">Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: J98-2005.xml | Citing Article: P01-1017.xml | Citation Marker Offset: ['79'] | Citation Marker: 5 | Citation Offset: ['79'] | Citation Text: <S sid ="79" ssid = "32">When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.</S> | Reference Offset: ['10'] | Reference Text: <S sid ="10" ssid = "10">This phenomenon is well known and well understood, and there are tests for &quot;tightness&quot; (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example Booth and Thompson [1973], Grenander [1976], and Harris [1963]).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: J98-2005.xml | Citing Article: P13-1102.xml | Citation Marker Offset: ['23'] | Citation Marker: 1998 | Citation Offset: ['23'] | Citation Text: <S sid ="23" ssid = "23">Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "29">Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the &quot;relative frequency&quot; estimator y&apos;~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production &quot;.&quot; in the tree w. The sentence a m, although ambiguous (there are multiple parses when m &gt; 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.</S> | Discourse Facet: BLANK | Annotator: Predictions

