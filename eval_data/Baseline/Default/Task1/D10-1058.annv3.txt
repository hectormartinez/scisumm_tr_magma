Citance Number: 1 | Reference Article: D10-1058 | Citing Article: C16-1060 | Citation Marker Offset: '48' | Citation Marker: 2010.0 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "27">Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '33' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '33' | Citation Text: <S sid ="33" ssid = "4">, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '43' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '43' | Citation Text: <S sid ="43" ssid = "14">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '82' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '82' | Citation Text: <S sid ="82" ssid = "3">Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['76'] | Reference Text: <S sid ="76" ssid = "28">In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '99' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '99' | Citation Text: <S sid ="99" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "14">Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I  J  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a P (a|a′) − 1)  2I +1 J  Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ  i=I +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = &quot;£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: D10-1058 | Citing Article: P59105ca | Citation Marker Offset: '45' | Citation Marker: 15.0 | Citation Offset: '45' | Citation Text: <S sid ="45" ssid = "45">Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.</S> | Reference Offset: ['118'] | Reference Text: <S sid ="118" ssid = "12">Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: D10-1058 | Citing Article: P87-94 | Citation Marker Offset: '79' | Citation Marker: Zhao | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "2">Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.</S> | Reference Offset: ['122'] | Reference Text: <S sid ="122" ssid = "16">For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '104' | Citation Marker: 2010.0 | Citation Offset: '103','104' | Citation Text: <S sid ="103" ssid = "24">For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).</S><S sid ="104" ssid = "25">134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '131' | Citation Marker: 2010.0 | Citation Offset: '131' | Citation Text: <S sid ="131" ssid = "13">Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: D10-1058 | Citing Article: Pcoling_D10 | Citation Marker Offset: '232' | Citation Marker: 2010.0 | Citation Offset: '232' | Citation Text: <S sid ="232" ssid = "9">Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '38' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '38' | Citation Text: <S sid ="38" ssid = "9">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.</S> | Reference Offset: ['78'] | Reference Text: <S sid ="78" ssid = "30">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '39' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '39' | Citation Text: <S sid ="39" ssid = "10">I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).</S> | Reference Offset: ['46'] | Reference Text: <S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '86' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "3">Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['76'] | Reference Text: <S sid ="76" ssid = "28">In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '104' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "14">Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I  J  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a P (a|a′) − 1)  2I +1 J  Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ  i=I +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = &quot;£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '60' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '60' | Citation Text: <S sid ="60" ssid = "3">The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).</S> | Reference Offset: ['25'] | Reference Text: <S sid ="25" ssid = "25">There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '130' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '130' | Citation Text: <S sid ="130" ssid = "22">Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "14">Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I  J  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a P (a|a′) − 1)  2I +1 J  Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ  i=I +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = &quot;£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.</S> | Discourse Facet: BLANK | Annotator: Predictions

