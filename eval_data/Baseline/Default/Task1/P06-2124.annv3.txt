Citance Number: 2 | Reference Article: P06-2124.txt | Citing Article: D11-1084.txt | Citation Marker Offset: ['50'] | Citation Marker: 2006 | Citation Offset: ['48','49','50','51'] | Citation Text: <S sid ="48" ssid = "1">There are only a few studies on document-level SMT.</S><S sid ="49" ssid = "2">Representative work includes Zhao et al.</S><S sid ="50" ssid = "3">(2006), Tam et al.</S><S sid ="51" ssid = "4">(2007), Carpuat (2009).</S> | Reference Offset: ['115'] | Reference Text: <S sid ="115" ssid = "25">The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: P06-2124.txt | Citing Article: D11-1084.txt | Citation Marker Offset: ['53'] | Citation Marker: 2006 | Citation Offset: ['52','53','54'] | Citation Text: <S sid ="52" ssid = "5">Zhao et al.</S><S sid ="53" ssid = "6">(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.</S><S sid ="54" ssid = "7">It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.</S> | Reference Offset: ['2'] | Reference Text: <S sid ="2" ssid = "2">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: P06-2124.txt | Citing Article: P07-1066.txt | Citation Marker Offset: ['28'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['28','29'] | Citation Text: <S sid ="28" ssid = "28">Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).</S><S sid ="29" ssid = "29">Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.</S> | Reference Offset: ['32'] | Reference Text: <S sid ="32" ssid = "2">Formally, we define the following terms1: • A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. • A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.• A document-pair (F, E) refers to two doc uments which are translations of each other.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: P06-2124.txt | Citing Article: P10-2025.txt | Citation Marker Offset: ['84'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['84'] | Citation Text: <S sid ="84" ssid = "22">We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).</S> | Reference Offset: ['71'] | Reference Text: <S sid ="71" ssid = "32">The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: P06-2124.txt | Citing Article: P10-2025.txt | Citation Marker Offset: ['86'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['86'] | Citation Text: <S sid ="86" ssid = "24">The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).</S> | Reference Offset: ['32'] | Reference Text: <S sid ="32" ssid = "2">Formally, we define the following terms1: • A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. • A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.• A document-pair (F, E) refers to two doc uments which are translations of each other.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: P06-2124.txt | Citing Article: P11-2032.txt | Citation Marker Offset: ['11'] | Citation Marker: 2006 | Citation Offset: ['11'] | Citation Text: <S sid ="11" ssid = "11">Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.</S> | Reference Offset: ['100'] | Reference Text: <S sid ="100" ssid = "10">where the Dirichlet parameter γ, the multinomial parameters (φ1, · · · , φn), and the parameters (ϕn1, · · · , ϕnJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(·) to the original p(·) via an iterative fixed-point algorithm.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: P06-2124.txt | Citing Article: P12-1048.txt | Citation Marker Offset: ['17'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['17'] | Citation Text: <S sid ="17" ssid = "17">Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.</S> | Reference Offset: ['113'] | Reference Text: <S sid ="113" ssid = "23">10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk &gt;0.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: P06-2124.txt | Citing Article: P12-1048.txt | Citation Marker Offset: ['149'] | Citation Marker: 2006 | Citation Offset: ['149'] | Citation Text: <S sid ="149" ssid = "13">Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.</S> | Reference Offset: ['2'] | Reference Text: <S sid ="2" ssid = "2">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: P06-2124.txt | Citing Article: P12-1048.txt | Citation Marker Offset: ['160'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['160'] | Citation Text: <S sid ="160" ssid = "24">â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.</S> | Reference Offset: ['113'] | Reference Text: <S sid ="113" ssid = "23">10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk &gt;0.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: P06-2124.txt | Citing Article: P12-1079.txt | Citation Marker Offset: ['8'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['8'] | Citation Text: <S sid ="8" ssid = "8">To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.</S> | Reference Offset: ['20'] | Reference Text: <S sid ="20" ssid = "20">Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: P06-2124.txt | Citing Article: P12-1079.txt | Citation Marker Offset: ['40'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['40'] | Citation Text: <S sid ="40" ssid = "1">Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).</S> | Reference Offset: ['113'] | Reference Text: <S sid ="113" ssid = "23">10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk &gt;0.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: P06-2124.txt | Citing Article: P12-2023.txt | Citation Marker Offset: ['30'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['30'] | Citation Text: <S sid ="30" ssid = "30">Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: P06-2124.txt | Citing Article: P13-2122.txt | Citation Marker Offset: ['27'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['27'] | Citation Text: <S sid ="27" ssid = "4">To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).</S> | Reference Offset: ['113'] | Reference Text: <S sid ="113" ssid = "23">10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk &gt;0.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: P06-2124.txt | Citing Article: W07-0722.txt | Citation Marker Offset: ['13'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['13','14','15'] | Citation Text: <S sid ="13" ssid = "13">In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.</S><S sid ="14" ssid = "14">These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.</S><S sid ="15" ssid = "15">The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.</S> | Reference Offset: ['40'] | Reference Text: <S sid ="40" ssid = "1">Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: P06-2124.txt | Citing Article: W07-0722.txt | Citation Marker Offset: ['62'] | Citation Marker: Zhao and Xing, 2006 | Citation Offset: ['62'] | Citation Text: <S sid ="62" ssid = "39">A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).</S> | Reference Offset: ['32'] | Reference Text: <S sid ="32" ssid = "2">Formally, we define the following terms1: • A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. • A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.• A document-pair (F, E) refers to two doc uments which are translations of each other.</S> | Discourse Facet: BLANK | Annotator: Predictions

