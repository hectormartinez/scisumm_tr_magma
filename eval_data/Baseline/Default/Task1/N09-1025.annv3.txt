Citance Number: 1 | Reference Article: N09-1025 | Citing Article: C10-2052 | Citation Marker Offset: '22' | Citation Marker: Chiang et al., 2009 | Citation Offset: '22' | Citation Text: <S sid ="22" ssid = "22">We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '6' | Citation Marker: Chiang et al., 2009 | Citation Offset: '6' | Citation Text: <S sid ="6" ssid = "6">Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '9' | Citation Marker: Chiang et al., 2009 | Citation Offset: '9' | Citation Text: <S sid ="9" ssid = "9">Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '25' | Citation Marker: Chiang et al., 2009 | Citation Offset: '25' | Citation Text: <S sid ="25" ssid = "25">(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '159' | Citation Marker: Chiang et al., 2009 | Citation Offset: '159' | Citation Text: <S sid ="159" ssid = "11">Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.</S> | Reference Offset: ['102'] | Reference Text: <S sid ="102" ssid = "34">We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f−1) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '239' | Citation Marker: Chiang et al., 2009 | Citation Offset: '239' | Citation Text: <S sid ="239" ssid = "91">Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '13' | Citation Marker: Chiang et al., 2009 | Citation Offset: '13' | Citation Text: <S sid ="13" ssid = "13">The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '78', '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '77', '78', '79' | Citation Text: <S sid ="77" ssid = "2">The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.</S>   <S sid ="78" ssid = "3">(2007) and Chiang et al.</S>   <S sid ="79" ssid = "4">(2008b; 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '208' | Citation Marker: Chiang et al., 2009 | Citation Offset: '207', '208', '209' | Citation Text: <S sid ="207" ssid = "55">We used the following feature classes in SBMT and PBMT extended scenarios:  Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1)  Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.</S> | Reference Offset: ['104'] | Reference Text: <S sid ="104" ssid = "36">(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '210' | Citation Marker: Chiang et al., 2009 | Citation Offset: '210', '211' | Citation Text: <S sid ="210" ssid = "58">Chiang et al. (2009), Section 4.1):10  Rule overlap features  Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT system—already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '227' | Citation Marker: Chiang et al., 2009 | Citation Offset: '227', '228', '229' | Citation Text: <S sid ="227" ssid = "16">5.4.1 MERT We used David Chiangs CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).</S> | Reference Offset: ['55'] | Reference Text: <S sid ="55" ssid = "28">, ein, which are the 10-best translations according to each of: h(e) · w B(e) + h(e) · w −B(e) + h(e) · w • For each i, select an oracle translation: (1) 4.1 Target-side.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: N12-1006 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102', '103' | Citation Text: <S sid ="102" ssid = "7">Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009) </S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "13">For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P12-1001 | Citation Marker Offset: '147' | Citation Marker: Chiang et al., 2009 | Citation Offset: '147' | Citation Text: <S sid ="147" ssid = "64">Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.</S> | Reference Offset: ['48'] | Reference Text: <S sid ="48" ssid = "21">To get more general translation rules, we restructure our English training trees using expectation- maximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '36' | Citation Marker: Chiang et al., 2009 | Citation Offset: '36' | Citation Text: <S sid ="36" ssid = "16">Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102' | Citation Text: <S sid ="102" ssid = "56">task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '115' | Citation Marker: Chiang et al., 2009 | Citation Offset: '115' | Citation Text: <S sid ="115" ssid = "9">The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '120' | Citation Marker: Chiang et al., 2009 | Citation Offset: '120' | Citation Text: <S sid ="120" ssid = "14">For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: N09-1025 | Citing Article: P28_n09 | Citation Marker Offset: '51' | Citation Marker: Chiang et al., 2009 | Citation Offset: '51' | Citation Text: <S sid ="51" ssid = "23">Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: N09-1025 | Citing Article: P134_n09 | Citation Marker Offset: '27' | Citation Marker: Chiang et al., 2009 | Citation Offset: '27', '28' | Citation Text: <S sid ="27" ssid = "27">First, we used features proposed by Chiang et al. (2009):  phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+)  target word insertion features  source word deletion features  word translation features  phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).</S> | Reference Offset: ['126'] | Reference Text: <S sid ="126" ssid = "16">We chose a stopping iteration based on the B score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3–5 −0.73 3 +0.54 6–10 −0.64 4 +0.29 5+ −0.02 Table 2: Weights learned for discount features.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '15' | Citation Marker: Chiang et al., 2009 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '16' | Citation Marker: Chiang et al., 2009 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '218' | Citation Marker: Chiang et al., 2009 | Citation Offset: '218' | Citation Text: <S sid ="218" ssid = "109">This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '245' | Citation Marker: Chiang et al., 2009 | Citation Offset: '245' | Citation Text: <S sid ="245" ssid = "136">The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '252' | Citation Marker: Chiang et al., 2009 | Citation Offset: '252' | Citation Text: <S sid ="252" ssid = "6">We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['39'] | Reference Text: <S sid ="39" ssid = "12">From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) ↔ x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: N09-1025 | Citing Article: PSMPT_n09 | Citation Marker Offset: '73' | Citation Marker: Chiang et al., 2009 | Citation Offset: '73', '74' | Citation Text: <S sid ="73" ssid = "6">Chiang et w(X  (, ,  )) = ii (2) i al. 2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.</S> | Reference Offset: ['104'] | Reference Text: <S sid ="104" ssid = "36">(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "79">For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.</S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "13">For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 14 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '86' | Citation Marker: Chiang et al., 2009 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "86">Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT system—already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '48' | Citation Marker: Chiang et al., 2009 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "25">When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '209' | Citation Marker: Chiang et al., 2009 | Citation Offset: '209' | Citation Text: <S sid ="209" ssid = "25">Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT system—already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '53' | Citation Marker: Chiang et al., 2009 | Citation Offset: '53', '54' | Citation Text: <S sid ="53" ssid = "30">Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.</S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "13">For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '92' | Citation Marker: Chiang et al., 2009 | Citation Offset: '92' | Citation Text: <S sid ="92" ssid = "25">A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.</S> | Reference Offset: ['212'] | Reference Text: <S sid ="212" ssid = "102">Bonus f e context −1.19 &lt;unk&gt; &lt;unk&gt; f−1 = ri ‘day’ −1.01 &lt;unk&gt; &lt;unk&gt; f−1 = ( −0.84 , that f−1 = shuo ‘say’ −0.82 yue ‘month’ &lt;unk&gt; f+1 = &lt;unk&gt; −0.78 &quot; &quot; f−1 = &lt;unk&gt; −0.76 &quot; &quot; f+1 = &lt;unk&gt; −0.66 &lt;unk&gt; &lt;unk&gt; f+1 = nian ‘year’ −0.65 , that f+1 = &lt;unk&gt; . P e n al ty f e context +1.12 &lt;unk&gt; ) f+1 = &lt;unk&gt; +0.83 jiang ‘shall’ be f+1 = &lt;unk&gt; +0.83 zhengfu ‘government’ the f−1 = &lt;unk&gt; +0.73 &lt;unk&gt; ) f−1 = &lt;unk&gt; +0.73 &lt;unk&gt; ( f+1 = &lt;unk&gt; +0.72 &lt;unk&gt; ) f−1 = ri ‘day’ +0.70 &lt;unk&gt; ( f−1 = ri ‘day’ +0.69 &lt;unk&gt; ( f−1 = &lt;unk&gt; +0.66 &lt;unk&gt; for f−1 = &lt;unk&gt; . +0.66 &lt;unk&gt; ’s f−1 = , +0.65 &lt;unk&gt; said f−1 = &lt;unk&gt; +0.60 , , f−1 = shuo ‘say’ . Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f−1 to the left or f+1 to the right.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 19 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '97' | Citation Marker: Chiang et al., 2009 | Citation Offset: '97' | Citation Text: <S sid ="97" ssid = "30">The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "7">Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

