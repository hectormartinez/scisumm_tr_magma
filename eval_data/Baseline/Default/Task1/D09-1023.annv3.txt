Citance Number: 1 | Reference Article: D09-1023 | Citing Article: D09-1086 | Citation Marker Offset: '78' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '78' | Citation Text: <S sid ="78" ssid = "14">In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Singapore, 67 August 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '1' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '1' | Citation Text: <S sid ="1" ssid = "1">We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '16' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '23' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '23' | Citation Text: <S sid ="23" ssid = "1">We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '41' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '41' | Citation Text: <S sid ="41" ssid = "17">We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Singapore, 67 August 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '146' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '146' | Citation Text: <S sid ="146" ssid = "6">For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.</S> | Reference Offset: ['99'] | Reference Text: <S sid ="99" ssid = "2">It equates to finding the most probable derivation under the s/τs-specific grammar Gs,τs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: D09-1023 | Citing Article: Pjournal | Citation Marker Offset: '79' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "18">(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Singapore, 67 August 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: D09-1023 | Citing Article: Pjournal | Citation Marker Offset: '139' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '139' | Citation Text: <S sid ="139" ssid = "78">Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: D09-1023 | Citing Article: Pproc_d09 | Citation Marker Offset: '205' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '205' | Citation Text: <S sid ="205" ssid = "24">Gimpel &amp; Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Singapore, 67 August 2009.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: D09-1023 | Citing Article: W10-1730-parscit | Citation Marker Offset: '17' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).</S> | Reference Offset: ['53'] | Reference Text: <S sid ="53" ssid = "21">(Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any func g (s, a, t) = Pm i∈a(j) f lex (si , tj ) (3) tion of words and alignments, we permit features + P f (slast (i,j) , tj ) that consider phrase pairs in which a target word g (t) = P i,j:1≤i&lt;j≤m Pm+1 phr first (i,j) i j lm N ∈{2,3} j=1 f N (tj−N +1 ) (4) outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, gsyn (t, τt ) = Pm j τ (j) , τt (j)) val t (j)) (5) 2007).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: D09-1023 | Citing Article: W11-2139 | Citation Marker Offset: '37' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '37' | Citation Text: <S sid ="37" ssid = "3">On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 20 | Reference Article: D09-1023 | Citing Article: N10-1040 | Citation Marker Offset: '6' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '6' | Citation Text: <S sid ="6" ssid = "6">Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).</S> | Discourse Facet: BLANK | Annotator: Predictions

