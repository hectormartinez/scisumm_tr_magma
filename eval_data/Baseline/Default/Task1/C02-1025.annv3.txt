Citance Number: 1 | Reference Article: C02-1025.txt | Citing Article: C10-2104.txt | Citation Marker Offset: ['115'] | Citation Marker: Chieu and Ng, 2002 | Citation Offset: ['115'] | Citation Text: <S sid ="115" ssid = "27">In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: C02-1025.txt | Citing Article: C10-2167.txt | Citation Marker Offset: ['65'] | Citation Marker: Chieu et al., 2002 | Citation Offset: ['65'] | Citation Text: <S sid ="65" ssid = "25">In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: C02-1025.txt | Citing Article: I05-3013.txt | Citation Marker Offset: ['88'] | Citation Marker: Chieu and Ng, 2002 | Citation Offset: ['88'] | Citation Text: <S sid ="88" ssid = "6">The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: C02-1025.txt | Citing Article: I05-3030.txt | Citation Marker Offset: ['33'] | Citation Marker: Chieu 2002 | Citation Offset: ['33'] | Citation Text: <S sid ="33" ssid = "33">modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).</S> | Reference Offset: ['7'] | Reference Text: <S sid ="7" ssid = "7">In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: C02-1025.txt | Citing Article: P02-1061.txt | Citation Marker Offset: ['51'] | Citation Marker: Chieu and Ng, 2002 | Citation Offset: ['51'] | Citation Text: <S sid ="51" ssid = "4">More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: C02-1025.txt | Citing Article: P03-1028.txt | Citation Marker Offset: ['161'] | Citation Marker: Chieu and Ng, 2002a | Citation Offset: ['161'] | Citation Text: <S sid ="161" ssid = "86">They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: C02-1025.txt | Citing Article: P03-1028.txt | Citation Marker Offset: ['52'] | Citation Marker: 2002a | Citation Offset: ['52'] | Citation Text: <S sid ="52" ssid = "29">Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: C02-1025.txt | Citing Article: P05-1045.txt | Citation Marker Offset: ['174'] | Citation Marker: 2002 | Citation Offset: ['174'] | Citation Text: <S sid ="174" ssid = "9">Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.</S> | Reference Offset: ['145'] | Reference Text: <S sid ="145" ssid = "85">With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 13 | Reference Article: C02-1025.txt | Citing Article: P05-1051.txt | Citation Marker Offset: ['27'] | Citation Marker: Chieu and Ng, 2002 | Citation Offset: ['27'] | Citation Text: <S sid ="27" ssid = "4">(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 15 | Reference Article: C02-1025.txt | Citing Article: W03-0423.txt | Citation Marker Offset: ['12'] | Citation Marker: Chieu and Ng, 2002b | Citation Offset: ['12'] | Citation Text: <S sid ="12" ssid = "12">Such global features enhance the performance of NER (Chieu and Ng, 2002b).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 16 | Reference Article: C02-1025.txt | Citing Article: W03-0423.txt | Citation Marker Offset: ['32'] | Citation Marker: Chieu and Ng, 2002a | Citation Offset: ['32'] | Citation Text: <S sid ="32" ssid = "8">Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 17 | Reference Article: C02-1025.txt | Citing Article: W03-0423.txt | Citation Marker Offset: ['44'] | Citation Marker: Chieu and Ng, 2002b | Citation Offset: ['44'] | Citation Text: <S sid ="44" ssid = "20">The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).</S> | Reference Offset: ['61'] | Reference Text: <S sid ="61" ssid = "1">The features we used can be divided into 2 classes: local and global.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 18 | Reference Article: C02-1025.txt | Citing Article: W03-0423.txt | Citation Marker Offset: ['62'] | Citation Marker: Chieu and Ng, 2002b | Citation Offset: ['62'] | Citation Text: <S sid ="62" ssid = "38">Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "26">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 19 | Reference Article: C02-1025.txt | Citing Article: W03-0423.txt | Citation Marker Offset: ['46'] | Citation Marker: Chieu and Ng, 2002b | Citation Offset: ['46'] | Citation Text: <S sid ="46" ssid = "22">In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).</S> | Reference Offset: ['52'] | Reference Text: <S sid ="52" ssid = "11">For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 20 | Reference Article: C02-1025.txt | Citing Article: W03-0432.txt | Citation Marker Offset: ['44'] | Citation Marker: Chieu and Ng, 2002a | Citation Offset: ['44'] | Citation Text: <S sid ="44" ssid = "7">Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 21 | Reference Article: C02-1025.txt | Citing Article: W04-0705.txt | Citation Marker Offset: ['8'] | Citation Marker: Chieu and Ng 2002 | Citation Offset: ['8'] | Citation Text: <S sid ="8" ssid = "8">AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 22 | Reference Article: C02-1025.txt | Citing Article: W04-0705.txt | Citation Marker Offset: ['147'] | Citation Marker: Chieu and Ng 2002 | Citation Offset: ['147'] | Citation Text: <S sid ="147" ssid = "30">Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 23 | Reference Article: C02-1025.txt | Citing Article: W06-0119.txt | Citation Marker Offset: ['11'] | Citation Marker: Chieu et al. 2002 | Citation Offset: ['11'] | Citation Text: <S sid ="11" ssid = "11">To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).</S> | Discourse Facet: BLANK | Annotator: Predictions

