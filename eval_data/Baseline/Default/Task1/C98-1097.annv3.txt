Citance Number: 1 | Reference Article: C98-1097 | Citing Article: C02-1033 | Citation Marker Offset: '15' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.</S> | Reference Offset: ['52'] | Reference Text: <S sid ="52" ssid = "8">Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 2 | Reference Article: C98-1097 | Citing Article: C02-1033 | Citation Marker Offset: '136' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '136' | Citation Text: <S sid ="136" ssid = "91">Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.</S> | Reference Offset: ['22'] | Reference Text: <S sid ="22" ssid = "22">It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 3 | Reference Article: C98-1097 | Citing Article: ICDAR99 | Citation Marker Offset: '31' | Citation Marker: 11.0 | Citation Offset: '31','32' | Citation Text: <S sid ="31" ssid = "1">In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.</S><S sid ="32" ssid = "2">To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.</S> | Reference Offset: ['45'] | Reference Text: <S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 4 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '17' | Citation Marker: Job- bins and Evett, 1998 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.</S> | Reference Offset: ['52'] | Reference Text: <S sid ="52" ssid = "8">Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 5 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '30' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '30','31','32' | Citation Text: <S sid ="30" ssid = "9">When no external knowledge is used, this similarity is only based on the strict reiteration of words.</S><S sid ="31" ssid = "10">But it can be enhanced by taking into account semantic relations between words.</S><S sid ="32" ssid = "11">This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.</S> | Reference Offset: ['58'] | Reference Text: <S sid ="58" ssid = "14">Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 6 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '90' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '89','90' | Citation Text: <S sid ="89" ssid = "5">The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.</S><S sid ="90" ssid = "6">This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).</S> | Reference Offset: ['47'] | Reference Text: <S sid ="47" ssid = "3">Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 7 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '98' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '97','98' | Citation Text: <S sid ="97" ssid = "13">This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.</S><S sid ="98" ssid = "14">As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "7">An inflected word was reduced to its stem by lookÂ­ up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. &quot;orange oranges&quot;).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 8 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '203' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '203','204' | Citation Text: <S sid ="203" ssid = "21">In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.</S><S sid ="204" ssid = "22">In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.</S> | Reference Offset: ['15'] | Reference Text: <S sid ="15" ssid = "15">Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 9 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '216' | Citation Marker: Job- bins and Evett, 1998 | Citation Offset: '216' | Citation Text: <S sid ="216" ssid = "8">This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">Text segmentation could also be used as a pre-processing step in automatic summarisation.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 10 | Reference Article: C98-1097 | Citing Article: P04470 | Citation Marker Offset: '27' | Citation Marker: 5.0 | Citation Offset: '27' | Citation Text: <S sid ="27" ssid = "4">In other words, meaning of UW can be found generally through co- occurrence words [5].</S> | Reference Offset: ['43'] | Reference Text: <S sid ="43" ssid = "43">A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 11 | Reference Article: C98-1097 | Citing Article: P06128 | Citation Marker Offset: '10' | Citation Marker: 1.0 | Citation Offset: '10' | Citation Text: <S sid ="10" ssid = "10">In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user&apos;s query can be retrieved.</S> | Discourse Facet: BLANK | Annotator: Predictions

Citance Number: 12 | Reference Article: C98-1097 | Citing Article: S0885 | Citation Marker Offset: '110' | Citation Marker: 1998.0 | Citation Offset: '109','110' | Citation Text: <S sid ="109" ssid = "27">Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.</S><S sid ="110" ssid = "28">These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "21">Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).</S> | Discourse Facet: BLANK | Annotator: Predictions

